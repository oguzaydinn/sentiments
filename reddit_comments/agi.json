{
  "the publicity from nvidia's historic one-day nasdaq decline last monday could generate billions of dollars for open source ai projects during the coming year": {
    "post_id": "1ifm3g4",
    "post_url": "https://www.reddit.com/r/agi/comments/1ifm3g4/the_publicity_from_nvidias_historic_oneday_nasdaq/",
    "num_comments": 1,
    "comments": [
      {
        "text": "People love scary things, so when they hear that USA and China are in an arms race they listen. The unfortunate part is that this exposes how incorrect the news cycle is and how we live in a world that people want to report first and best at any cost rather than giving the proper time to report accurately.\n\nHow did our culture evolve to a point where we appreciated truth in the news, to this cluster fuck? I guess too many megaphones?\n\nAnyhow, I agree, this will be a net positive... just like Kanye West... AI got attention, and that equals investments.",
        "score": 3
      }
    ]
  },
  "¼ of Humanity's Last Exam conquered! Within a Month !": {
    "post_id": "1igc0pe",
    "post_url": "https://www.reddit.com/r/agi/comments/1igc0pe/¼_of_humanitys_last_exam_conquered_within_a_month/",
    "num_comments": 2,
    "comments": [
      {
        "text": "inb4 they admit they actually trained on answers for it + spent thousands in compute letting it brute force it",
        "score": 6
      },
      {
        "text": "probably overfitting.\n\nAlso not GI related at all",
        "score": 2
      }
    ]
  },
  "the accelerating pace of ai releases. how much faster when the giants start using deepseek's rl hybrid method?": {
    "post_id": "1iati2z",
    "post_url": "https://www.reddit.com/r/agi/comments/1iati2z/the_accelerating_pace_of_ai_releases_how_much/",
    "num_comments": 2,
    "comments": [
      {
        "text": "no one knows, and it's probably the wrong question to be asking rn.  ive seen no interesting progress on any of the questions i care about.  the models still are training to beat benchmarks that we all know they're gonna beat one way or another.  there aren't any interesting benchmarks that people are developing.  they still can't do long horizon planning, and that might require something completely different in terms of RL than deepseek's methods.  they need to be putting effort into solving the questions we will demand the models solve after they've crunched through all our small textbook problems.  things that require big context, non-hallucinatory mathematical abilities, the use of tools, development of tools, abstraction, etc.  \n\ni think most people ought to be bored by what these companies have chosen to do with their research money.",
        "score": 4
      },
      {
        "text": "Only benchmark I care about is whether or not ai irrecoverably breaks capitalism",
        "score": 5
      }
    ]
  },
  "The value of AIs that exclusively use logic and reasoning to the goal of AGI": {
    "post_id": "1aienv0",
    "post_url": "https://www.reddit.com/r/agi/comments/1aienv0/the_value_of_ais_that_exclusively_use_logic_and/",
    "num_comments": 5,
    "comments": [
      {
        "text": "-1 you seem to be confused and also confuse \"free will\" / causality with determinism and predictability etc. .\n\nHumans and AI can have free will even if the basic mechanisms are following rigid rules. Why? Because rigid rules can be very flexible. Best example here is the unpredictable behavior of game if life from very simple rules. \n\nhttps://www.youtube.com/watch?v=m3DX5kMfkaI\n\nThe unpredictable behavior from simple rules is also known as emergence\n\nhttps://m.youtube.com/watch?v=16W7c0mb-rE\n\nAll complex systems (not to be confused with complicated systems) have emergence.\n\nhttps://en.m.wikipedia.org/wiki/Emergence",
        "score": 5
      },
      {
        "text": "> a major problem with this approach is that we humans tend to get a lot wrong. if we're to achieve AGI, we need to correct this.\n\nWhy?  Do humans not exhibit general intelligence, despite getting a lot wrong?",
        "score": 3
      },
      {
        "text": "Scale seems to improve things. Small LLMs show the syptoms you describe, they hallucinate, or give imperfect answeres, possibly the most popular ones. But once the the LLM reaches a certain scale, it seems to do much better, like by having more examples to think with, even if many of them are average, it's capable to reason better. And come to think about it, it's interesting that what gave place to this LLM stuff if the ability to predict the next letter, or word. The analysis of the chains of cause and effects, of taking in account the historical context of things, has turned out to be of great importance.",
        "score": 1
      },
      {
        "text": "We have free will of the kind that matters. See the writings of Daniel Dennett and Sean Carroll. Regardless of whether the universe is determined or not, we make decisions based on a function of input and our current state, just like a thermostat but more complicated. We are responsible for the decisions we make. Free will and responsibility are cultural concepts.",
        "score": 1
      },
      {
        "text": "You have strange ideas about AI.\n\nWhy would anyone believe transformers are built to reason? \n\nThey predict what people are most likely to say after a prompt.",
        "score": 1
      }
    ]
  }
}