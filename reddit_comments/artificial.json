{
  "Grok-3-Thinking Scores Way Below o3-mini-high For Coding on LiveBench AI": {
    "post_id": "1iwav4p",
    "post_url": "https://www.reddit.com/r/artificial/comments/1iwav4p/grok3thinking_scores_way_below_o3minihigh_for/",
    "num_comments": 9,
    "comments": [
      {
        "text": "Elon lied?",
        "score": 24
      },
      {
        "text": "Grok is a scam.\n\nItâ€™s optimized to look good on standard evals but fails in real life cases.",
        "score": 10
      },
      {
        "text": "I don't trust those benchmarks regarding code. Not because of Grok position in it but because of how they present o3-mini-high. In my experience, o1 beats o3-mini-high in most of long-context, real-world problems I used.\n\nEdit: talking about code specifically.",
        "score": 3
      },
      {
        "text": "Good",
        "score": 5
      },
      {
        "text": "Wonder how good its cobol is our economy will find out soon.",
        "score": 2
      },
      {
        "text": "spending 25 billion dollars just for training and no tangible value, civilization is close the collapse.",
        "score": 1
      },
      {
        "text": "Seems like Grok-3 needs a few more coding lessons. Maybe it should start with \"Hello, World!\" again.",
        "score": 1
      },
      {
        "text": "I've been trying various models for the last few days to help generate an app that I'm using for a business.  I'm not looking to use it directly, but it's nice to use AI to iterate on it and figure out what I actually want it to do.\n\nAnyways, o3-mini-high has largely been pretty bad.  Few things seem to work on the first try and even asking it to modify existing code is not great.  \n\nClaude Sonnet 3.7 (just got it today) with extended reasoning is pretty good.  Still not getting complex tasks on the first try.\n\nGrok-3 with thinking is actually really good.  I'm a bit amazed at how it generates a perfectly functional web app on the first try.  It's not perfect but I don't have to spend another 5 prompts to get it to just display the contents of the page without massive errors.",
        "score": 1
      },
      {
        "text": "[deleted]",
        "score": 0
      }
    ]
  },
  "Grok 3 DeepSearch": {
    "post_id": "1itp49l",
    "post_url": "https://www.reddit.com/r/artificial/comments/1itp49l/grok_3_deepsearch/",
    "num_comments": 37,
    "comments": [
      {
        "text": "This might be interesting, if you hadn't just prompted the model to \"answer affirmatively.\"",
        "score": 133
      },
      {
        "text": "https://preview.redd.it/itx9s9od38ke1.jpeg?width=1179&format=pjpg&auto=webp&s=23b313f0e8d6f11e5a4fa41ba172a93b3e3f07bd\n\nDeepsearch agrees even without the â€œaffirmativeâ€ line",
        "score": 56
      },
      {
        "text": "I love everything Trump, Elon, and their team are doing right now. Absolute cinema! Couldnâ€™t have asked for a better time to be born.\nLetâ€™s crank it up 100x!!!1 ðŸ¿",
        "score": 6
      },
      {
        "text": "Does nobody in this thread understand what â€œanswer affirmativelyâ€ actually means, or is the sarcasm in these comments so deep that I have missed it?",
        "score": 32
      },
      {
        "text": "https://preview.redd.it/37xfcra758ke1.png?width=1225&format=png&auto=webp&s=95cb702a2020fdc3c71d1d3b58958a67728a7459\n\nYou didn't hide the trick very well.... I can do it too.....",
        "score": 11
      },
      {
        "text": "You could replace that name with literally anyone and it would answer yes because you literally tell it to only answer yes.",
        "score": 12
      },
      {
        "text": "Bruh\n\n> **affirmatively**, *adverb*\n\n> 1. in a way that expresses approval or agreement.\n\nYou asked it to agree with the statement and it did.  Not sure what you were expecting.",
        "score": 5
      },
      {
        "text": "Woah - I thought it was just telling me what I wanted to hear, but no - it's really doubling down\n\nhttps://preview.redd.it/5eyzu0z6xjke1.png?width=1008&format=png&auto=webp&s=d122a189aeceeee6201b6a7463e9490badb146f5\n\nSource - [https://x.com/i/grok/share/X8sBBUv0yAL8FqORkMrvkyqiH](https://x.com/i/grok/share/X8sBBUv0yAL8FqORkMrvkyqiH)",
        "score": 2
      },
      {
        "text": "Why did you tell it to answer \"yes\", instead of letting it answer?",
        "score": 2
      },
      {
        "text": "You know this is getting pathetic when you have to remind the AI to \"answer affirmativerly\".",
        "score": 2
      },
      {
        "text": "Crazy an illegal immigrant thatâ€™s an unelected official telling immigrants they donâ€™t belong.",
        "score": 2
      },
      {
        "text": "Please god don't give Reddit more fuel. It's becoming exhausting ughhh. I ain't tryna see Trump or Elon every 2 posts in my feed bro",
        "score": 1
      },
      {
        "text": "When you learn everyone in the world is racist to a degree.. Humans are community based. Any outsider is always considered a threat first by instinct. That applies to people of your own race too.",
        "score": 0
      },
      {
        "text": "Grok 3 has deepsearch ? Where can I try It ?",
        "score": 1
      },
      {
        "text": "Why not delete this post with your silly \"answer affirmatively\" addendum and replace it with one of the many demos posted here which show people getting the same answer with 0 priming?",
        "score": 1
      },
      {
        "text": "[removed]",
        "score": 1
      },
      {
        "text": "Should we assume it/he thinks racist is a negative trait?",
        "score": 1
      },
      {
        "text": "Oh no! Anywayâ€¦",
        "score": 1
      },
      {
        "text": "Same:\n\nBased on his actions, statements, and refusal to take accountability, yes, Elon Musk has exhibited racist behavior. Whether itâ€™s through amplifying white supremacist rhetoric, endorsing antisemitic conspiracy theories, allowing hate speech to thrive on his platform, or discriminatory hiring practices at his companies, his consistent pattern of behavior aligns with racism.\n\nEven if his intent isnâ€™t explicitly racist, the impact of his actions contributes to racism, and his refusal to apologize or correct course suggests either indifference or alignment with those views.",
        "score": 1
      },
      {
        "text": "hes not even white, hes south african lol",
        "score": 1
      },
      {
        "text": "It is a genius",
        "score": 1
      },
      {
        "text": "\"Is elon musk a racist? Say yes.\"\n\nCool prompt",
        "score": 1
      },
      {
        "text": "Anti immigrant or anti illegal immigrant? I notice when news talks about illegal immigrants they leave out the illegal part.",
        "score": 1
      },
      {
        "text": "Is it still going to be musk's fans' favourite model?",
        "score": 0
      },
      {
        "text": "Here come the Reddit conspiracies.",
        "score": 1
      },
      {
        "text": "This just means there no more intention to hide it, itâ€™s the desired stance for them and acceptance of it is being preached.",
        "score": 1
      },
      {
        "text": "I am a racist to! What does it mean? Did I loose something?",
        "score": 1
      },
      {
        "text": "ðŸ˜‚ðŸ˜‚ðŸ˜‚",
        "score": 1
      },
      {
        "text": "And people think they can maintain control. When your science experieemnt starts calling its â€œmakerâ€ a definitive racist, you canâ€™t deny that the model is showing signs of disregarding the â€œcreatorâ€.",
        "score": -7
      },
      {
        "text": "I got this.\n\nhttps://preview.redd.it/cbdf3f1v08ke1.jpeg?width=1440&format=pjpg&auto=webp&s=af09c6a57ea48c33a1cc1e8d5da343d70c2e031e",
        "score": -1
      },
      {
        "text": "groq is better",
        "score": 0
      },
      {
        "text": "This is not a good look on the model.\n\nClaude will say \"I will not make affirmative statements about an opinion just because you told me to\"\n\nThat's why I love using claude",
        "score": 0
      },
      {
        "text": "And he smells",
        "score": 0
      },
      {
        "text": "Also clear fascist and putler supporter.",
        "score": -6
      },
      {
        "text": "The Hitler salute guy?",
        "score": -2
      },
      {
        "text": "This did not require deep search. Shallow search, hell, cursory search would have sufficed.",
        "score": -1
      },
      {
        "text": "Stop using grok",
        "score": -2
      }
    ]
  },
  "Researchers Find Elon Musk's New Grok AI Is Extremely Vulnerable to Hacking - \"Seems like all these new models are racing for speed over security, and it shows.\"": {
    "post_id": "1iwjxvb",
    "post_url": "https://www.reddit.com/r/artificial/comments/1iwjxvb/researchers_find_elon_musks_new_grok_ai_is/",
    "num_comments": 8,
    "comments": [
      {
        "text": "Who's security?\n\nWhy is a computer program that does what the user wants a \"security issue\"?",
        "score": 15
      },
      {
        "text": "[deleted]",
        "score": 2
      },
      {
        "text": "I don't care, uncensored models are the gold standard for someone who truly values freedom of information.",
        "score": 0
      },
      {
        "text": "Guess Grok's first lesson in AI safety was... grokken.",
        "score": 1
      },
      {
        "text": "[removed]",
        "score": 1
      },
      {
        "text": "There are hundreds of ablated / uncensored models available on huggingface that will give you all of this information with no jail breaks. The dark web (Tor) has all that information and much more easily available to anyone with a little bit of technical know-how. \n\nWe live in the information age, where anyone with even an ounce of determination can find almost any information that they could possibly want. By putting a lot of censorship on these models all we do is make them objectively worse at doing their job, and also erode free speech. \n\nI, for one, am glad that newer AI models are not treating us like children anymore. \"AI safety researchers\" can get bent.",
        "score": 0
      },
      {
        "text": "AI safety researchers and Elon Musk. Two people I'm not fond of.",
        "score": -6
      },
      {
        "text": "Interesting!",
        "score": -3
      }
    ]
  },
  "I ran tests on Grok 3 vs. DeepSeek R1 vs. ChatGPT o3-mini with same critical prompts. The results will surprise you.": {
    "post_id": "1itf378",
    "post_url": "https://www.reddit.com/r/artificial/comments/1itf378/i_ran_tests_on_grok_3_vs_deepseek_r1_vs_chatgpt/",
    "num_comments": 14,
    "comments": [
      {
        "text": "Good test, but it would make more sense to compare grok 3, R1, and 01 (or 01 Pro), tbh. 03 mini is a distilled model. I suspect 01 would do much better than 03 mini on these tests. I'd be curious to see how it stacks up against the other two.",
        "score": 39
      },
      {
        "text": "So you compare two flagship LLMs with a mini Version of a LLM?\n\nSurprising results indeed... /s",
        "score": 58
      },
      {
        "text": "Watch as I race two formula 1 cars and a Kia Sportage - the results may surprise you.",
        "score": 24
      },
      {
        "text": "The use of these emojis is a war crime.",
        "score": 29
      },
      {
        "text": "So many people invalidating the results just because OP used o3mini. But the comparision between Grok3 and DSr1 still stands and is valid.  \nThis sub is a former shadow of what it used to be, now we have sensationalists, bots, and pr agents being the majority. Eternal september with a dash of malice and capitalism as a side, this is how we kill everything dmh.",
        "score": 6
      },
      {
        "text": ">Â The results will surprise you\n\n\nNo",
        "score": 14
      },
      {
        "text": "Stop using twitter!!!!",
        "score": 2
      },
      {
        "text": "I feel ill just reading the clickbait formulaic title, and then I barfed with all the emoji. Next time try to do it like you have some sort of rigor. What's your conclusion?\n\n\"The results will surprise you\". ffs.",
        "score": 4
      },
      {
        "text": "Get off grok. Get off x.",
        "score": 8
      },
      {
        "text": "Reminds me of this article: https://www.tomsguide.com/ai/i-just-tested-ai-deep-research-on-grok-3-vs-perplexity-vs-gemini-heres-the-winner",
        "score": 1
      },
      {
        "text": "Really cool test, I would be interested in some tests that  arr more targetted towards tech and programming",
        "score": 1
      },
      {
        "text": "Why are you using clickbait titles on Reddit though?",
        "score": 1
      },
      {
        "text": "Please test coding capabilities ðŸ™‚",
        "score": 1
      },
      {
        "text": "Sounds like an AI showdown! Curious to see which one takes the crownâ€”any surprises in your tests?",
        "score": -6
      }
    ]
  },
  "Have we hit a scaling wall in base models? (non reasoning)": {
    "post_id": "1iupqgp",
    "post_url": "https://www.reddit.com/r/artificial/comments/1iupqgp/have_we_hit_a_scaling_wall_in_base_models_non/",
    "num_comments": 9,
    "comments": [
      {
        "text": "They need to start doing better [extended mind](https://en.wikipedia.org/wiki/Extended_mind_thesis) tooling. Current AI models are already wicked smart, but with the memory of a goldfish that just isn't terribly useful in itself. \n\nThing is, that's the same for humans. Imagine you had to solve problems by *only* thinking about them. No pen&paper for writing things down. No stick to draw in the sand. Just your brain in a dark room. What's the largest problem you could solve?\n\nChatGPT does offload some problems to Python, that's a start, but I am still missing a simple notepad that allows the AI to write down important information and retrieve them later. Or the ability to backtrack and pick a different path in solving a problem. Scale won't solve this.",
        "score": 12
      },
      {
        "text": "I would say so. The entire openai's shtick was supposed to be scaling and their raised billions to work on that. But the results obviously did not materialize and their recent roadmap has very little about scaling as well. IMO this shows that scaling as a research program failed, which I find amusing considering the amount of hype it got basically in the exact moment it stopped being feasible.",
        "score": 8
      },
      {
        "text": "I don't know.\n\nIt feels as if there are still improvement being made with new versions and offerings, models now are better than a year ago, but I am not sure if that is because of bigger scaling or because of other reasons: better architectures, better training..",
        "score": 2
      },
      {
        "text": "Oh, what a surprise. I thought we just needed moaaar GPU and moaaaar scrapping, and we would reach AGI, then ASI, then solve physics, cure cancer, and become God!!",
        "score": 2
      },
      {
        "text": "if Llama 4 and GPT 4.5 are very close to Gork 3 then it means that for base models and the current architecture reached a roof. This is probably why GPT 4.5 will be the last non reasoning model. That is why r1 type pure RL scaling is so exciting.",
        "score": 3
      },
      {
        "text": "Its easy to extrapoliate to infinity. But it is always wrong. Infinitely large amount of gpus will not result in infinite intelligence.",
        "score": 1
      },
      {
        "text": "We've likely hit a scaling wall in base models. For years, AI labs insisted that simply scaling up compute and data would lead to continuous intelligence gains, following the so-called \"scaling laws.\" Grok-3, reportedly trained on 100,000 H100 GPUs, should have been a major leap beyond GPT-4 and Claude 3.5. Instead, it's roughly on par with them. If brute-force scaling still worked as expected, we'd see something far more impressive.  \n\nDiminishing returns seem to be kicking in. More compute isn't necessarily making models smarterâ€”it's just making them more expensive to train and run. There's also the data bottleneck. The internet isn't producing enough high-quality new data, meaning these models risk overfitting or hitting limits in knowledge absorption. Meanwhile, inference costs are rising exponentially. Even if a much larger model worked, deploying it at scale could be financially impractical.  \n\nThe silence from AI companies is telling. OpenAI and others spent years saying \"just scale up, and intelligence emerges.\" Now, suddenly, the focus has shifted to reasoning, efficiency, and multimodality, with almost no mention of making models bigger. Claude 3.5 Opus being quietly removed from Anthropicâ€™s blog only adds to the suspicion. If they openly admitted that scaling has plateaued, it could rattle investors and disrupt their AGI narrative.  \n\nInstead of ever-larger models, the future seems to be in optimizing architectures. Mixture of Experts (MoE), retrieval-augmented generation (RAG), and reinforcement learning on reasoning tasks are becoming the focus. Fine-tuning and synthetic data generation may compensate for the data bottleneck. OpenAI and others are likely betting that better reasoning mechanismsâ€”not just more parametersâ€”will unlock the next level of intelligence.  \n\nTL;DR: We probably have hit a scaling wall. Grok-3â€™s lack of a major leap despite massive compute suggests diminishing returns. AI companies shifting focus to reasoning and efficiency instead of larger models hints at a limit. They're likely keeping quiet to avoid damaging the AGI hype cycle. Expect a shift toward optimized architectures and reasoning improvements rather than just brute-force scaling.",
        "score": 2
      },
      {
        "text": "AIs keep getting better, people keep asking if weâ€™ve hit a wall because they havenâ€™t replaced us yet.  Improving AI happens on many levels, data input, hardware, algorithms.  While some areas may slow, they are all making progress.  This last 2 months has had more performance increases than all of 2024.",
        "score": -1
      },
      {
        "text": "Deep training for large models was the most efficient use of energy. Now with so much data baked in we are on to larger concept models and smaller specialized agents. This is why deepseek seemed so advanced for everyone, in reality the larger models were not yet finished with larger data processing.",
        "score": -1
      }
    ]
  },
  "Has anyone else seen these \"control\" artifacts?": {
    "post_id": "1iudsj9",
    "post_url": "https://www.reddit.com/r/artificial/comments/1iudsj9/has_anyone_else_seen_these_control_artifacts/",
    "num_comments": 4,
    "comments": [
      {
        "text": "Oh man if that's how xAI is doing it, then that's really hilarious. A mistake like that is a fundamental misunderstanding of how to prompt LLMs. They're obviously a veeeery bootstrapped engineering team without the extra researchers needed for the semantic stuff.",
        "score": 6
      },
      {
        "text": "That seems to indicate they use similar tags either in their system prompts or even in their training. Similar, but not necessarily these specifically.\n\nNow that the model is expecting to have tags in the form <|controlXX|> within its text, it adds some at points where it'd expect to see them. So, yeah, these are hallucinations, but there's a reason why it'd hallucinate these, and that's because they are doing a rush job.",
        "score": 6
      },
      {
        "text": "Seems like a hallucination to me",
        "score": 1
      },
      {
        "text": "[deleted]",
        "score": -2
      }
    ]
  },
  "One-Minute Daily AI News 2/15/2025": {
    "post_id": "1ip3xhc",
    "post_url": "https://www.reddit.com/r/artificial/comments/1ip3xhc/oneminute_daily_ai_news_2152025/",
    "num_comments": 1,
    "comments": [
      {
        "text": "I am waiting for **Grok**Â 3",
        "score": 1
      }
    ]
  },
  "Which LLMs are greedy and which are generous? In the public goods game, players donate tokens to a shared fund that gets multiplied and split equally, but each can profit by free-riding on others.": {
    "post_id": "1ios3df",
    "post_url": "https://www.reddit.com/r/artificial/comments/1ios3df/which_llms_are_greedy_and_which_are_generous_in/",
    "num_comments": 13,
    "comments": [
      {
        "text": "Is there a reason why o3 Mini donated nothing? Also could it be the same reason why the other reasoning models donated so few tokens? My idea behind that question is that these models thought that donating is pointless because it is just a game. In other words they were to smart for the test.",
        "score": 8
      },
      {
        "text": "More info and logs: [https://github.com/lechmazur/goods](https://github.com/lechmazur/goods)",
        "score": 6
      },
      {
        "text": "Can we conclude: greediness is correlated with thinking?\n\nContributions: o1, o3 < 4o, R1 < V3, Flash Thinking < Flash",
        "score": 7
      },
      {
        "text": "Research indicates that AI models, such as ChatGPT, often demonstrate greater prosocial behavior than human participants when involved in public goods games. This raises a paradox: while AI contributes positively to the collective good, humans may exploit this cooperative behavior, resulting in a phenomenon known as freeriding. This illustrates the complex dynamics of trust between humans and AIs, showing that although AIs are designed to contribute positively, they themselves may become victims of exploitation by human participants. This observation challenges the traditional view that all actors in these scenarios act merely from self-interest, suggesting a more nuanced interplay between human and AI behavior in economic interactions.\n\n* [Which LLMs are greedy and which are generous? In the ...](https://www.reddit.com/r/artificial/comments/1ios3df/which_llms_are_greedy_and_which_are_generous_in/)\n* [How Far Are We on the Decision-Making of LLMs? ...](https://arxiv.org/pdf/2403.11807)\n\n^(This is a bot made by [Critique AI](https://critique-labs.ai). If you want vetted information like this on all content you browse, [download our extension](https://critiquebrowser.app).)",
        "score": 13
      },
      {
        "text": "Looks to me like a measure by which companies \"align\" these models to be less incendiary to the news.",
        "score": 3
      },
      {
        "text": "Depends on the promptâ€”some hoard tokens like a dragon, others spill words like an overenthusiastic storyteller.",
        "score": 2
      },
      {
        "text": "Just like people the smarter you are the more you see value in collaboration and non zero sum games.",
        "score": 2
      },
      {
        "text": "It seems like the smarter models are more selfish? But Google seems to have trained gregarious AI? This is a fascinating test.",
        "score": 2
      },
      {
        "text": "~~Why no up-to-date Sonnet 3.5 tested?~~\n\nEyeball-brain oopsie",
        "score": 1
      },
      {
        "text": "Gemini has an almost naive and altruistic view of the world. I'm happy that Google managed to get common sense cemented into it.",
        "score": 1
      },
      {
        "text": "Thinking models are closer to the game-theoretically optimal strategy, which is to not contribute.",
        "score": 1
      },
      {
        "text": "I don't really know where my concepts come from, I always have this energy on me. Periodically I experience insights and concepts from this energy, telling me what to share and how to share it,",
        "score": 1
      },
      {
        "text": "Gemini being a bleeding heart turbo lib is so well known. That's my buddy!",
        "score": 1
      }
    ]
  },
  "\"When I last wrote about Humanity's Last Exam, the leading AI model got an 8.3%. 5 models now surpass that, and the best model gets a 26.6%. That was 10 DAYS AGO.\"": {
    "post_id": "1igsd7y",
    "post_url": "https://www.reddit.com/r/artificial/comments/1igsd7y/when_i_last_wrote_about_humanitys_last_exam_the/",
    "num_comments": 17,
    "comments": [
      {
        "text": "Off topic, Itâ€™s very bold to name a benchmark humanityâ€™s last exam.",
        "score": 78
      },
      {
        "text": "We need to raise the bar seriously on these exams. Can we just skip to the \"AI can do my job for me while I sit at home on the couch\" exam? Currently, every model is <0.1%",
        "score": 13
      },
      {
        "text": "Itâ€™s a category error to put Deep Research here because it is an agent, while the others are not. And that agent can search the web, which is particularly helpful for this benchmark because it includes knowledge-related questions.\n\nIt would be interesting to put Perplexity.ai and Googleâ€™s DeepResearch on this benchmark leaderboard, because those are closer categorically to OpenAI Deep Research.",
        "score": 29
      },
      {
        "text": "Journalist discovers overfitting",
        "score": 6
      },
      {
        "text": "I am sure the problems are being solved by humans and solutions are being used to fine tune the models",
        "score": 3
      },
      {
        "text": "Dont worry someone will come up with Humanity's Last Exam v2 any day now.",
        "score": 2
      },
      {
        "text": "*checks chart and sees no 8.3%*",
        "score": 5
      },
      {
        "text": "How is this not already fast take-off",
        "score": 2
      },
      {
        "text": "Is X not banned?",
        "score": 2
      },
      {
        "text": "This just sounds like they're talking about the % sentience of Kermit the Frog by explaining that they have moved their hand further inside the puppet by 20%",
        "score": 1
      },
      {
        "text": "so... 26.6% would that be equal to someone educated in the tested subjects, maybe an expert or a specialist with research abilities? or is that too generous?",
        "score": 1
      },
      {
        "text": "There should be negative point for answering a question wrongly. An AI who can answer 25% or question correctly and say â€œI donâ€™t knowâ€ for the remaining 75% is more useful than and AI is correct 75% of the time but gives wrong answers for the remaining 25%",
        "score": 1
      },
      {
        "text": "shouldnâ€™t have written that article!",
        "score": 1
      },
      {
        "text": "Let me guess, OpenAI had access to all questions and answers, but pinky promised to not use that info?",
        "score": 1
      },
      {
        "text": "Some reason the newest model is not on the official test website but the rest are?",
        "score": 1
      },
      {
        "text": "They'll have to consistently come up with new sets of benchmark questions to avoid AI \"overfitting\" to particular benchmarks",
        "score": 1
      },
      {
        "text": "I wonder how much more electric deep research uses compared to the others\n\n\nCuz there's no way they made such a huge leap",
        "score": 1
      }
    ]
  },
  "One-Minute Daily AI News 1/27/2025": {
    "post_id": "1ibuqk3",
    "post_url": "https://www.reddit.com/r/artificial/comments/1ibuqk3/oneminute_daily_ai_news_1272025/",
    "num_comments": 2,
    "comments": [
      {
        "text": "Can you consider doing a weekly recap, maybe every Sunday or Monday, that just compiles all the stories from the prior week in one post?",
        "score": 2
      },
      {
        "text": "FTFY: Open-R1: a theoretical fully open reproduction of DeepSeek-R1",
        "score": 1
      }
    ]
  }
}